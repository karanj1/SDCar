{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, Activation, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('data/data/driving_log.csv') as csvfile:\n",
    "    #csvlines = csvfile.readlines()  # if we use this method then comment below 3 lines and replace samples by csvlines\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        if float(line[3])>0.0001 or float(line[3])<-0.0001:\n",
    "            samples.append(line)\n",
    "        else:\n",
    "            select_prob = np.random.random()\n",
    "            if select_prob > 0.9:\n",
    "                samples.append(line)\n",
    "            \n",
    "        #samples.append(line)\n",
    "        \n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print (len(samples), len(train_samples), len(validation_samples), samples[550])\n",
    "a= np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(a[:,3].astype(np.float), color = 'b', label = 'train', normed = False, bins=120)\n",
    "plt.title('Distribution of the number of steering angles')\n",
    "plt.xlabel('Angles')\n",
    "plt.ylabel('no of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def brightness_process_image(Xpp_in, Xpp_out):\n",
    "    \n",
    "    #Brightness normalization\n",
    "    cv2.normalize(Xpp_in, Xpp_out, 30, 225, cv2.NORM_MINMAX)\n",
    "    \n",
    "    return Xpp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#car_images = []\n",
    "#steering_angles = []\n",
    "i=0\n",
    "\n",
    "def process_image(image):\n",
    "    \n",
    "    shape = image.shape\n",
    "    # note: numpy arrays are (row, col)!\n",
    "    image = image[math.floor(shape[0]/4):shape[0]-25, 0:shape[1]]\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    \n",
    "    #image2 = np.zeros(image.shape, dtype='u1')\n",
    "    #image2 = brightness_process_image(image, image2)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sample processing on first image in csv file\n",
    "path = 'data/data/' # fill in the path to your training IMG directory\n",
    "\n",
    "divider = np.asarray(np.zeros((64,10,3), dtype='u1'))\n",
    "\n",
    "#img_center1 = process_image(np.asarray(Image.open(path+csvlines[550][0:38]))) #line[x][0:38], x is row number, [0:38] is characters of first coulmn of that row..\n",
    "img_center1 = process_image(np.asarray(Image.open(path+samples[250][0])))  #line[x][1], x is row number, [0] is 0th coulmn of that row..\n",
    "print(img_center1.shape, divider.shape)\n",
    "img_center1_flipped = np.fliplr(img_center1)\n",
    "res1 = np.hstack((img_center1, divider, img_center1_flipped))\n",
    "plt.imshow(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Augmentation(row, car_images, steering_angles):\n",
    "    steering_center = float(row[3])\n",
    "\n",
    "    # create adjusted steering measurements for the side camera images\n",
    "    correction = 0.12     # this is a parameter to tune\n",
    "    steering_left = steering_center + correction\n",
    "    steering_right = steering_center - correction\n",
    "    \n",
    "    # read in images from center, left and right cameras\n",
    "    path = 'data/data/' # fill in the path to your training IMG directory\n",
    "    img_center = process_image(np.asarray(Image.open(path+row[0])))\n",
    "    img_center_flipped = np.fliplr(img_center)\n",
    "    img_left = process_image(np.asarray(Image.open(path+row[1])))\n",
    "    img_left_flipped = np.fliplr(img_left)\n",
    "    img_right = process_image(np.asarray(Image.open(path + row[2])))\n",
    "    img_right_flipped = np.fliplr(img_right)\n",
    "    \n",
    "    # add images and angles to data set\n",
    "    car_images.append(img_center)\n",
    "    car_images.append(img_center_flipped)\n",
    "    car_images.append(img_left)\n",
    "    car_images.append(img_left_flipped)\n",
    "    car_images.append(img_right)\n",
    "    car_images.append(img_right_flipped)\n",
    "    steering_angles.append(steering_center)\n",
    "    steering_angles.append(-steering_center)\n",
    "    steering_angles.append(steering_left)\n",
    "    steering_angles.append(-steering_left)\n",
    "    steering_angles.append(steering_right)\n",
    "    steering_angles.append(-steering_right)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Sample example\n",
    "\n",
    "Augmentation(a[300])\n",
    "for i in range(6):\n",
    "    image = car_images[i]\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.tight_layout()\n",
    "    #print(steering_angles[i])\n",
    "    plt.imshow(image, aspect='auto')\n",
    "    plt.title(steering_angles[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = int(len(samples)/3)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            car_images = []\n",
    "            steering_angles = []\n",
    "\n",
    "            for row in batch_samples:\n",
    "                Augmentation(row, car_images, steering_angles)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(car_images)\n",
    "            y_train = np.array(steering_angles)\n",
    "            #print(X_train.shape, y_train.shape)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model = Sequential()\n",
    "#model.add(Cropping2D(cropping=((40,25),(0,0)), input_shape=(160, 320, 3)))\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(64,64,3)))\n",
    "\n",
    "model.add(Convolution2D(3,1,1,border_mode='valid', name='conv0'))\n",
    "model.add(Convolution2D(16,3,3, activation='relu', name='conv1'))   #62x62x16 #80x300x16\n",
    "model.add(MaxPooling2D((2, 2), border_mode='valid', name='pool1'))  #31x31x16 #40x150x16\n",
    "                        \n",
    "model.add(Convolution2D(32,8,8, activation='relu'))   #24x24x32 #30x130x32\n",
    "model.add(MaxPooling2D((2, 2), border_mode='valid'))  #13x13x32 #15x65x32\n",
    "                        \n",
    "model.add(Flatten())\n",
    "                        \n",
    "model.add(Dense(256))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(0.5))\n",
    "          \n",
    "model.add(Dense(100))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(0.5))\n",
    "                        \n",
    "model.add(Dense(30))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(64,64,3)))\n",
    "# layer 1 output shape is 32x32x32\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=(64, 64, 3), subsample=(2, 2), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "# layer 2 output shape is 15x15x16\n",
    "model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(.4))\n",
    "model.add(MaxPooling2D((2, 2), border_mode='valid'))\n",
    "# layer 3 output shape is 12x12x16\n",
    "model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(.4))\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "# layer 4\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(.3))\n",
    "model.add(ELU())\n",
    "# layer 5\n",
    "model.add(Dense(512))\n",
    "model.add(ELU())\n",
    "# Finally a single output, since this is a regression problem\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras import backend as K\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "#model.fit(X_train, y_train,validation_split=0.2, shuffle=True, nb_epoch=7)\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples)*2, validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples)*2, nb_epoch=10, verbose=1)         #https://keras.io/models/sequential/#fit_generator\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model.save('model_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
